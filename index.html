<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hanchen Cui</title>

    <meta name="author" content="Hanchen Cui">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hanchen Cui
                </p>
                <p>I am a Ph.D. student in computer science at <a href="https://cse.umn.edu/cs">University of Minnesota Twin Cities</a>,
                   working with Prof. <a href="https://karthikdesingh.com/">Karthik Desingh</a>. I received my master's and bachelor's degree 
                   from <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a> and <a href="https://en.xidian.edu.cn/">Xidian University</a>, respectively. Previously, 
                   I'm a researcher at Shanghai Qi Zhi Institute, where I work closely with 
                   Prof. <a href="https://yang-gao.weebly.com/">Yang Gao</a> at <a href="https://iiis.tsinghua.edu.cn/en/">IIIS, Tsinghua University</a>. 
                   Afterwards, I have a short visit at <a href="https://nus.edu.sg/">NUS</a>, 
                   collaborating with Prof. <a href="https://linsats.github.io/">Lin Shao</a>
                </p>
                <p>
                  My research interest lies in <strong>robot learning</strong> and my long-term goal is 
                  to build <strong>a generalized robot</strong> that can accomplish long-horizon and complex 
                  tasks. To be mroe specific, I work on <strong>vision-language-action models</strong>, 
                  <strong>world models</strong>, <strong>representation learning</strong>, and <strong>legged locomotion</strong>.
                </p>
                <p>
                  I am interning at <a href="https://ai.meta.com/research/">Meta FAIR</a> in the summer of 2025, working on world models and vision-language-action models.
                </p>
                <p>
                  <strong>Email:</strong> hanchen.cui147[at]gmail.com
                </p>
                <!-- <p>
                  <span style="color: red;">
                    I am actively seeking internship opportunities for Summer 2025.
                  </span>
                  If you're interested in collaborating, feel free to reach out!
                </p> -->
                <p style="text-align:center">
                  <a href="mailto:hanchen.cui147@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=vc_x1E0AAAAJ&hl=en&oi=ao">Scholar</a>  &nbsp;/&nbsp;
				          <a href="https://www.linkedin.com/in/hanchen-c-3a6686343/">LinkedIn</a> 
				  <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jonbarron/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Hanchen.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Hanchen.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2> <p>(representative papers are <span class="highlight">highlighted</span>)</p>
                <!-- <p>
                  I'm interested in computer vision, machine learning, optimization, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/hard2sim.png" alt="clean-usnob" width="320" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="https://hard-to-sim.github.io/">
                  <span class="papertitle">Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving </span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?hl=en&user=I0HLZAwAAAAJ">Ruiqian Nai</a>, <a href="https://github.com/YouJiacheng">Jiacheng You</a>, <a href="https://github.com/xiaohu-art">Liu Cao</a>, <strong>Hanchen Cui</strong>, <a href="https://hard-to-sim.github.io/">Shiyuan Zhang</a>, <a href="http://hxu.rocks/">Huazhe Xu</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                <br>
                ICRA 2025
                <br>
                <a href="https://hard-to-sim.github.io/">project page</a> /
                <a href="https://hard-to-sim.github.io/static/root.pdf">pdf</a>
                <p>We propose a data-driven framework for fine-tuning locomotion policies, targeting these <strong>hard-to-simulate objectives</strong>. Our framework leverages real-world data to model these objectives and incorporates the learned model into simulation for policy improvement.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/concept.png" alt="clean-usnob" width="320" height="112">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/pdf?id=lw5GlytIY5">
                  <span class="papertitle">A Universal World Model Learned from Large Scale and Diverse Videos</span>
                </a>
                <br>
                <strong>Hanchen Cui</strong>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                <br>
                <em>NeurIPS</em> 2023, Foundation Models for Decision Making Workshop
                <br>
                <a href="https://openreview.net/pdf?id=lw5GlytIY5">pdf</a> /
                <a href="https://drive.google.com/file/d/1cJRLMlkC2s4FbOA2sBLbbKrO9SO2Fuk5/view?usp=sharing">poster</a>
                <p>We propose <strong>a generalizable world model</strong> pre-tained by large scale and diverse video dataset and then fine-tune the world model to obtain an accurate dynamic function in a sample-efficient manner.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/SGR.png" alt="clean-usnob" width="320" height="128">
              </td>
              <td width="75%" valign="middle">
                <a href="https://semantic-geometric-representation.github.io/">
                  <span class="papertitle">A Universal Semantic-Geometric Representation for Robotic Manipulation</span>
                </a>
                <br>
                <a href="https://tongzhangthu.github.io/">Tong Zhang*</a>, <a href="https://yingdong-hu.github.io/">Yingdong Hu*</a>, <strong>Hanchen Cui</strong>,<a href="https://hangzhaomit.github.io/">Hang Zhao</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                <br>
                <em>CoRL</em> 2023
                <br>
                <a href="https://semantic-geometric-representation.github.io/">project page</a> /
                <a href="https://arxiv.org/abs/2306.10474">arXiv</a> /
                <a href="https://github.com/TongZhangTHU/sgr">code</a>
                <p>We present <strong>Semantic-Geometric Representation (SGR)</strong>, a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning.</p>
              </td>
            </tr>            



          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Research projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            


            <tr onmouseout="robot_stop()" onmouseover="robot_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='robot_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/robot_manipulation_cmd.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/robot.png' width="260" height="180">
                </div>
                <script type="text/javascript">
                  function robot_start() {
                    document.getElementById('robot_image').style.opacity = "1";
                  }
        
                  function robot_stop() {
                    document.getElementById('robot_image').style.opacity = "0";
                  }
                  robot_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">A vision-language intelligent manipulation system</span>
                <br>
                <p>
                  (1) Build a vision-language manipulation deployment system to accomplish a variety of tasks based on Franka Panda Robot.
                </p>
                <p>
                  (2) A robust client-server architecture is employed to enable low-latency remote control..
                </p>
                <p>
                  (3) Fine-tune the vision-language manipulation model(CLIPORT), achieving 90% success rate in real-world experiments.
                </p>
              </td>
            </tr>







            <tr onmouseout="legged_stop()" onmouseover="legged_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='real_world_learning'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/legged_est_new.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/real world learning.png' width="320" height="180">
                </div>
                <script type="text/javascript">
                  function legged_start() {
                    document.getElementById('real_world_learning').style.opacity = "1";
                  }
        
                  function legged_stop() {
                    document.getElementById('real_world_learning').style.opacity = "0";
                  }
                  legged_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Legged robots learn from physical world</span>
                <br>
                <p>
                  (1) Establish a sim2real locomotion deployment codebase for legged robots.
                </p>
                <p>
                  (2) Develop a real-world learning framework that acquires data from real-world 
                  interactions and performs policy training on a remote server.
                </p>
              </td>
            </tr> 
            


            
            <tr onmouseout="nvg_stop()" onmouseover="nvg_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='visual-planning'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/move2chair_tp.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/visual-planning.png' width="320" height="160">
                </div>
                <script type="text/javascript">
                  function nvg_start() {
                    document.getElementById('visual-planning').style.opacity = "1";
                  }
        
                  function nvg_stop() {
                    document.getElementById('visual-planning').style.opacity = "0";
                  }
                  nvg_stop()
                </script>
              </td> 
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Visual planning for legged robots</span>
                <br>
                <p>
                  (1) Leverage the strong image understanding and high-level planning ability of <strong>large multimodal
                  models</strong>(like GPT4o), enabling legged robots to accomplish multi-step and complex tasks,
                  such as sending packages, and going out to find an object.
                </p>
              </td>
            </tr>


            
            <!-- <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/FDP.png" alt="foundation policy model" width="1" height="1">
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle"> </span>
              </td>
            </tr>             -->

            <!-- <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/FDP.png" alt="foundation policy model" width="320" height="160">
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Foundation policy model for robotic manipulation (on-going)</span>
                <br>
                <p>
                  (1) Build a foundation policy model from large-scale expert dataset like Open X-Embodiment.
                </p>
                <p>
                  (2) Extract and label action squences using large vision-language models.
                </p>
                <p>
                  (3) Training and inference use action-aware sequence data instead of MDP.
                </p>
              </td>
            </tr> -->

            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Modified version of template from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
